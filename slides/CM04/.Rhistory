}
close(pb)
df = data.frame(cbind(error_lda, error_lrg, error_nvb, error_qda))
boxplot(df)
# Load required packages
library("MASS")
library("e1071")
library("class")
# Set seed for reproducibility
set.seed(42)
# Define function for generating data in Scenario 1
# (eps controls how hard the clf problem is)
generate_data_scenario_1 <- function(nsamples, eps)
{
ndim = 2
m0 = c(0, 0)
s0 = rbind(c(1, 0), c(0, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x0 = (z + m0) %*% chol(s0)
m1 = eps * c(1, 1) / sqrt(2)
s1 = s0
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x1 = (z + m1) %*% chol(s1)
X = rbind(x0, x1)
y = c(rep(0, nsamples), rep(1, nsamples))
df = data.frame(cbind(X, y))
colnames(df) <- c('X1', 'X2', 'y')
return(df)
}
# Define function for generating data in Scenario 2
# (eps controls how hard the clf problem is)
generate_data_scenario_2 <- function(nsamples, eps)
{
rho = -0.5
ndim = 2
m0 = c(0, 0)
s0 = rbind(c(1, sqrt(2)*rho), c(sqrt(2)*rho, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x0 = (z + m0) %*% chol(s0)
m1 = eps * c(1, 1) / sqrt(2)
s1 = rbind(c(1, sqrt(2)*rho), c(sqrt(2)*rho, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x1 = (z + m1) %*% chol(s1)
X = rbind(x0, x1)
y = c(rep(0, nsamples), rep(1, nsamples))
df = data.frame(cbind(X, y))
colnames(df) <- c('X1', 'X2', 'y')
return(df)
}
# Define function for generating data in Scenario 3
# (eps controls how hard the clf problem is)
generate_data_scenario_3 <- function(nsamples, eps)
{
rho = -0.5
df = 10
ndim = 2
m0 = c(0, 0)
s0 = rbind(c(1, sqrt(2)*rho), c(sqrt(2)*rho, 2))
z = matrix(rt(nsamples*ndim, df=df), nrow=nsamples, ncol=ndim)
x0 = (z + m0) %*% chol(s0)
m1 = eps * c(1, 1) / sqrt(2)
s1 = rbind(c(1, sqrt(2)*rho), c(sqrt(2)*rho, 2))
z = matrix(rt(nsamples*ndim, df=df), nrow=nsamples, ncol=ndim)
x1 = (z + m1) %*% chol(s1)
X = rbind(x0, x1)
y = c(rep(0, nsamples), rep(1, nsamples))
df = data.frame(cbind(X, y))
colnames(df) <- c('X1', 'X2', 'y')
return(df)
}
# Define function for generating data in Scenario 4
# (eps controls how hard the clf problem is)
generate_data_scenario_4 <- function(nsamples, eps)
{
rho_0 = +0.5
rho_1 = -0.5
ndim = 2
m0 = c(0, 0)
s0 = rbind(c(1, sqrt(2)*rho_0), c(sqrt(2)*rho_0, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x0 = (z + m0) %*% chol(s0)
m1 = eps * c(1, 1) / sqrt(2)
s1 = rbind(c(1, sqrt(2)*rho_1), c(sqrt(2)*rho_1, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x1 = (z + m1) %*% chol(s1)
X = rbind(x0, x1)
y = c(rep(0, nsamples), rep(1, nsamples))
df = data.frame(cbind(X, y))
colnames(df) <- c('X1', 'X2', 'y')
return(df)
}
# Define function for generating data in Scenario 5
# (eps controls how hard the clf problem is)
generate_data_scenario_5 <- function(nsamples, eps)
{
ndim = 2
m0 = c(0, 0)
s0 = rbind(c(2, 0), c(0, 4))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x0 = (z + m0) %*% chol(s0)
m1 = eps * c(1, 1) / sqrt(2)
s1 = rbind(c(5, 0), c(0, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x1 = (z + m1) %*% chol(s1)
X = rbind(x0, x1)
y = c(rep(0, nsamples), rep(1, nsamples))
df = data.frame(cbind(X, y))
colnames(df) <- c('X1', 'X2', 'y')
return(df)
}
# choose scenario
generate_data = generate_data_scenario_2
# Decide the difficulty of the classification problem
eps = 1.25
# # Plot the data from the choosen scenario
# df_plot = generate_data(nsamples=1000, eps=eps)
# X = df_plot[,1:2]
# y = df_plot[,3]
# plot(X, col='white', xlab="X1", ylab="X2")
# points(X[y == 0,], col='#1f77b4', pch=16)
# points(X[y == 1,], col='#ff7f0e', pch=16)
# Size of the training dataset
ntrain = 20
# Size of the test dataset
ntest = 5000
# How many repetitions to consider
nrzt = 100
# Run the loop
pb = txtProgressBar(min = 0, max = nrzt, initial = 0)
error_lda <- c()
error_lrg <- c()
error_nvb <- c()
error_qda <- c()
for (i in 1:nrzt){
# print(i)
setTxtProgressBar(pb,i)
# Generate training and test data
df_train = generate_data(nsamples=ntrain, eps=eps)
df_test = generate_data(nsamples=ntest, eps=eps)
# LDA classification
ldaclf = lda(y ~ ., data=df_train)
y_pred_lda = predict(ldaclf, newdata=df_test)
error_lda_i = sum(y_pred_lda$class != df_test[,3]) / (2*ntest)
error_lda = c(error_lda, error_lda_i)
# Logistic regression
logregclf <- glm(y ~ ., data=df_train, family=binomial)
y_pred_logreg <- predict.glm(logregclf, newdata=df_test, type="response")
error_lrg_i = sum((y_pred_logreg > 0.5) != df_test[,3])/(2*ntest)
error_lrg = c(error_lrg, error_lrg_i)
# Gaussian naive Bayes
naivclf <- naiveBayes(y ~ ., data=df_train)
y_pred_naive <- predict(naivclf, newdata=df_test, type="raw")
error_nvb_i = sum((y_pred_naive[,1] > 0.5) == df_test[,3]) / (2*ntest)
error_nvb = c(error_nvb, error_nvb_i)
# QDA classification
qdaclf = qda(y ~ ., data=df_train)
y_pred_qda = predict(qdaclf, newdata=df_test)
error_qda_i = sum(y_pred_qda$class != df_test[,3]) / (2*ntest)
error_qda = c(error_qda, error_qda_i)
}
close(pb)
df = data.frame(cbind(error_lda, error_lrg, error_nvb, error_qda))
boxplot(df)
# Load required packages
library("MASS")
library("e1071")
library("class")
# Set seed for reproducibility
set.seed(42)
# Define function for generating data in Scenario 1
# (eps controls how hard the clf problem is)
generate_data_scenario_1 <- function(nsamples, eps)
{
ndim = 2
m0 = c(0, 0)
s0 = rbind(c(1, 0), c(0, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x0 = (z + m0) %*% chol(s0)
m1 = eps * c(1, 1) / sqrt(2)
s1 = s0
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x1 = (z + m1) %*% chol(s1)
X = rbind(x0, x1)
y = c(rep(0, nsamples), rep(1, nsamples))
df = data.frame(cbind(X, y))
colnames(df) <- c('X1', 'X2', 'y')
return(df)
}
# Define function for generating data in Scenario 2
# (eps controls how hard the clf problem is)
generate_data_scenario_2 <- function(nsamples, eps)
{
rho = -0.5
ndim = 2
m0 = c(0, 0)
s0 = rbind(c(1, sqrt(2)*rho), c(sqrt(2)*rho, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x0 = (z + m0) %*% chol(s0)
m1 = eps * c(1, 1) / sqrt(2)
s1 = rbind(c(1, sqrt(2)*rho), c(sqrt(2)*rho, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x1 = (z + m1) %*% chol(s1)
X = rbind(x0, x1)
y = c(rep(0, nsamples), rep(1, nsamples))
df = data.frame(cbind(X, y))
colnames(df) <- c('X1', 'X2', 'y')
return(df)
}
# Define function for generating data in Scenario 3
# (eps controls how hard the clf problem is)
generate_data_scenario_3 <- function(nsamples, eps)
{
rho = -0.5
df = 10
ndim = 2
m0 = c(0, 0)
s0 = rbind(c(1, sqrt(2)*rho), c(sqrt(2)*rho, 2))
z = matrix(rt(nsamples*ndim, df=df), nrow=nsamples, ncol=ndim)
x0 = (z + m0) %*% chol(s0)
m1 = eps * c(1, 1) / sqrt(2)
s1 = rbind(c(1, sqrt(2)*rho), c(sqrt(2)*rho, 2))
z = matrix(rt(nsamples*ndim, df=df), nrow=nsamples, ncol=ndim)
x1 = (z + m1) %*% chol(s1)
X = rbind(x0, x1)
y = c(rep(0, nsamples), rep(1, nsamples))
df = data.frame(cbind(X, y))
colnames(df) <- c('X1', 'X2', 'y')
return(df)
}
# Define function for generating data in Scenario 4
# (eps controls how hard the clf problem is)
generate_data_scenario_4 <- function(nsamples, eps)
{
rho_0 = +0.5
rho_1 = -0.5
ndim = 2
m0 = c(0, 0)
s0 = rbind(c(1, sqrt(2)*rho_0), c(sqrt(2)*rho_0, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x0 = (z + m0) %*% chol(s0)
m1 = eps * c(1, 1) / sqrt(2)
s1 = rbind(c(1, sqrt(2)*rho_1), c(sqrt(2)*rho_1, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x1 = (z + m1) %*% chol(s1)
X = rbind(x0, x1)
y = c(rep(0, nsamples), rep(1, nsamples))
df = data.frame(cbind(X, y))
colnames(df) <- c('X1', 'X2', 'y')
return(df)
}
# Define function for generating data in Scenario 5
# (eps controls how hard the clf problem is)
generate_data_scenario_5 <- function(nsamples, eps)
{
ndim = 2
m0 = c(0, 0)
s0 = rbind(c(2, 0), c(0, 4))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x0 = (z + m0) %*% chol(s0)
m1 = eps * c(1, 1) / sqrt(2)
s1 = rbind(c(5, 0), c(0, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x1 = (z + m1) %*% chol(s1)
X = rbind(x0, x1)
y = c(rep(0, nsamples), rep(1, nsamples))
df = data.frame(cbind(X, y))
colnames(df) <- c('X1', 'X2', 'y')
return(df)
}
# choose scenario
generate_data = generate_data_scenario_3
# Decide the difficulty of the classification problem
eps = 1.25
# # Plot the data from the choosen scenario
# df_plot = generate_data(nsamples=1000, eps=eps)
# X = df_plot[,1:2]
# y = df_plot[,3]
# plot(X, col='white', xlab="X1", ylab="X2")
# points(X[y == 0,], col='#1f77b4', pch=16)
# points(X[y == 1,], col='#ff7f0e', pch=16)
# Size of the training dataset
ntrain = 20
# Size of the test dataset
ntest = 5000
# How many repetitions to consider
nrzt = 100
# Run the loop
pb = txtProgressBar(min = 0, max = nrzt, initial = 0)
error_lda <- c()
error_lrg <- c()
error_nvb <- c()
error_qda <- c()
for (i in 1:nrzt){
# print(i)
setTxtProgressBar(pb,i)
# Generate training and test data
df_train = generate_data(nsamples=ntrain, eps=eps)
df_test = generate_data(nsamples=ntest, eps=eps)
# LDA classification
ldaclf = lda(y ~ ., data=df_train)
y_pred_lda = predict(ldaclf, newdata=df_test)
error_lda_i = sum(y_pred_lda$class != df_test[,3]) / (2*ntest)
error_lda = c(error_lda, error_lda_i)
# Logistic regression
logregclf <- glm(y ~ ., data=df_train, family=binomial)
y_pred_logreg <- predict.glm(logregclf, newdata=df_test, type="response")
error_lrg_i = sum((y_pred_logreg > 0.5) != df_test[,3])/(2*ntest)
error_lrg = c(error_lrg, error_lrg_i)
# Gaussian naive Bayes
naivclf <- naiveBayes(y ~ ., data=df_train)
y_pred_naive <- predict(naivclf, newdata=df_test, type="raw")
error_nvb_i = sum((y_pred_naive[,1] > 0.5) == df_test[,3]) / (2*ntest)
error_nvb = c(error_nvb, error_nvb_i)
# QDA classification
qdaclf = qda(y ~ ., data=df_train)
y_pred_qda = predict(qdaclf, newdata=df_test)
error_qda_i = sum(y_pred_qda$class != df_test[,3]) / (2*ntest)
error_qda = c(error_qda, error_qda_i)
}
close(pb)
df = data.frame(cbind(error_lda, error_lrg, error_nvb, error_qda))
boxplot(df)
# Load required packages
library("MASS")
library("e1071")
library("class")
# Set seed for reproducibility
set.seed(42)
# Define function for generating data in Scenario 1
# (eps controls how hard the clf problem is)
generate_data_scenario_1 <- function(nsamples, eps)
{
ndim = 2
m0 = c(0, 0)
s0 = rbind(c(1, 0), c(0, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x0 = (z + m0) %*% chol(s0)
m1 = eps * c(1, 1) / sqrt(2)
s1 = s0
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x1 = (z + m1) %*% chol(s1)
X = rbind(x0, x1)
y = c(rep(0, nsamples), rep(1, nsamples))
df = data.frame(cbind(X, y))
colnames(df) <- c('X1', 'X2', 'y')
return(df)
}
# Define function for generating data in Scenario 2
# (eps controls how hard the clf problem is)
generate_data_scenario_2 <- function(nsamples, eps)
{
rho = -0.5
ndim = 2
m0 = c(0, 0)
s0 = rbind(c(1, sqrt(2)*rho), c(sqrt(2)*rho, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x0 = (z + m0) %*% chol(s0)
m1 = eps * c(1, 1) / sqrt(2)
s1 = rbind(c(1, sqrt(2)*rho), c(sqrt(2)*rho, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x1 = (z + m1) %*% chol(s1)
X = rbind(x0, x1)
y = c(rep(0, nsamples), rep(1, nsamples))
df = data.frame(cbind(X, y))
colnames(df) <- c('X1', 'X2', 'y')
return(df)
}
# Define function for generating data in Scenario 3
# (eps controls how hard the clf problem is)
generate_data_scenario_3 <- function(nsamples, eps)
{
rho = -0.5
df = 10
ndim = 2
m0 = c(0, 0)
s0 = rbind(c(1, sqrt(2)*rho), c(sqrt(2)*rho, 2))
z = matrix(rt(nsamples*ndim, df=df), nrow=nsamples, ncol=ndim)
x0 = (z + m0) %*% chol(s0)
m1 = eps * c(1, 1) / sqrt(2)
s1 = rbind(c(1, sqrt(2)*rho), c(sqrt(2)*rho, 2))
z = matrix(rt(nsamples*ndim, df=df), nrow=nsamples, ncol=ndim)
x1 = (z + m1) %*% chol(s1)
X = rbind(x0, x1)
y = c(rep(0, nsamples), rep(1, nsamples))
df = data.frame(cbind(X, y))
colnames(df) <- c('X1', 'X2', 'y')
return(df)
}
# Define function for generating data in Scenario 4
# (eps controls how hard the clf problem is)
generate_data_scenario_4 <- function(nsamples, eps)
{
rho_0 = +0.5
rho_1 = -0.5
ndim = 2
m0 = c(0, 0)
s0 = rbind(c(1, sqrt(2)*rho_0), c(sqrt(2)*rho_0, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x0 = (z + m0) %*% chol(s0)
m1 = eps * c(1, 1) / sqrt(2)
s1 = rbind(c(1, sqrt(2)*rho_1), c(sqrt(2)*rho_1, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x1 = (z + m1) %*% chol(s1)
X = rbind(x0, x1)
y = c(rep(0, nsamples), rep(1, nsamples))
df = data.frame(cbind(X, y))
colnames(df) <- c('X1', 'X2', 'y')
return(df)
}
# Define function for generating data in Scenario 5
# (eps controls how hard the clf problem is)
generate_data_scenario_5 <- function(nsamples, eps)
{
ndim = 2
m0 = c(0, 0)
s0 = rbind(c(2, 0), c(0, 4))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x0 = (z + m0) %*% chol(s0)
m1 = eps * c(1, 1) / sqrt(2)
s1 = rbind(c(5, 0), c(0, 2))
z = matrix(rnorm(nsamples*ndim, mean=0, sd=1), nrow=nsamples, ncol=ndim)
x1 = (z + m1) %*% chol(s1)
X = rbind(x0, x1)
y = c(rep(0, nsamples), rep(1, nsamples))
df = data.frame(cbind(X, y))
colnames(df) <- c('X1', 'X2', 'y')
return(df)
}
# choose scenario
generate_data = generate_data_scenario_4
# Decide the difficulty of the classification problem
eps = 1.25
# # Plot the data from the choosen scenario
# df_plot = generate_data(nsamples=1000, eps=eps)
# X = df_plot[,1:2]
# y = df_plot[,3]
# plot(X, col='white', xlab="X1", ylab="X2")
# points(X[y == 0,], col='#1f77b4', pch=16)
# points(X[y == 1,], col='#ff7f0e', pch=16)
# Size of the training dataset
ntrain = 20
# Size of the test dataset
ntest = 5000
# How many repetitions to consider
nrzt = 100
# Run the loop
pb = txtProgressBar(min = 0, max = nrzt, initial = 0)
error_lda <- c()
error_lrg <- c()
error_nvb <- c()
error_qda <- c()
for (i in 1:nrzt){
# print(i)
setTxtProgressBar(pb,i)
# Generate training and test data
df_train = generate_data(nsamples=ntrain, eps=eps)
df_test = generate_data(nsamples=ntest, eps=eps)
# LDA classification
ldaclf = lda(y ~ ., data=df_train)
y_pred_lda = predict(ldaclf, newdata=df_test)
error_lda_i = sum(y_pred_lda$class != df_test[,3]) / (2*ntest)
error_lda = c(error_lda, error_lda_i)
# Logistic regression
logregclf <- glm(y ~ ., data=df_train, family=binomial)
y_pred_logreg <- predict.glm(logregclf, newdata=df_test, type="response")
error_lrg_i = sum((y_pred_logreg > 0.5) != df_test[,3])/(2*ntest)
error_lrg = c(error_lrg, error_lrg_i)
# Gaussian naive Bayes
naivclf <- naiveBayes(y ~ ., data=df_train)
y_pred_naive <- predict(naivclf, newdata=df_test, type="raw")
error_nvb_i = sum((y_pred_naive[,1] > 0.5) == df_test[,3]) / (2*ntest)
error_nvb = c(error_nvb, error_nvb_i)
# QDA classification
qdaclf = qda(y ~ ., data=df_train)
y_pred_qda = predict(qdaclf, newdata=df_test)
error_qda_i = sum(y_pred_qda$class != df_test[,3]) / (2*ntest)
error_qda = c(error_qda, error_qda_i)
}
close(pb)
df = data.frame(cbind(error_lda, error_lrg, error_nvb, error_qda))
boxplot(df)
eps = 1.4
eps = 1.5
m1 = eps * c(1, 1) / sqrt(2)
m1
load('food.rda')
setwd("~/Courses/ISLA/local/ISLA-2024/website-main/slides/CM04")
load('food.rda')
df = foodOfTheWorld$compact.CT
filename = 'biplot-food-01.pdf'
pdf(filename, height=9.9, width=16.8)
pca.food = prcomp(df, center=TRUE)
biplot(pca.food)
dev.off()
filename = 'biplot-food-02.pdf'
pdf(filename, height=9.9, width=16.8)
pca.food = prcomp(df, center=TRUE, scale=TRUE)
biplot(pca.food)
dev.off()
filename = 'biplot-food-03.pdf'
pdf(filename, height=9.9, width=16.8)
df_norm = df /rowSums(df)
pca.food = prcomp(df_norm, center=TRUE)
biplot(pca.food)
dev.off()
df
dim(df)
sum(df)
sum(df)/26
df
dim(df)
df
