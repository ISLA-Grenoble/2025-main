---
# title: "Template Work/labsheet"
# author: "Christophe Dutang, Pedro Rodrigues"
documentclass: article
papersize: a4
geometry: top=1.5cm, bottom=2cm, left=1.5cm, right=1.5cm
fontsize: 11pt
output: 
  pdf_document:
    extra_dependencies: ["enumitem"]
    number_sections: true
    toc: false
    keep_tex: false
    includes:
      in_header: "TD3-preamble.tex"
      before_body: "TD3-header.tex"
      
---

<!-- see help at https://bookdown.org/yihui/rmarkdown-cookbook/latex-output.html -->

```{r setup, include=FALSE, message=FALSE}
#see full list >knitr::opts_chunk$get()
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
library(reticulate)
use_python("/opt/homebrew/Caskroom/miniforge/base/envs/isla2025/bin/python", required = T)
```

\section*{$\blacktriangleright$~Exercise 1}
Consider dataset $\mathcal{D} = \Big\{(\mathbf{x}_i, y_i)\Big\}_{i = 1}^{i = N}$ with $\mathbf{x}_i \in \mathbb{R}^p$ and $y_i \in \mathbb{R}$.

\begin{enumerate}
\item[(a)] Show that
\begin{equation*}
\dfrac{1}{N}\sum_{i = 1}^N \|\mathbf{x}_i - \bar{\mathbf{x}}\|^2 = \text{tr}(\mathbf{\Sigma})
\end{equation*}
where $\bar{\mathbf{x}}_i$ is the average of the samples of the dataset and $\mathbf{\Sigma}$ is their sample covariance matrix.
\item[(b)] Show that if the samples are standardized (i.e. they have zero mean and unit standard deviation) then
\begin{equation*}
\dfrac{1}{N}\sum_{i = 1}^N \|\mathbf{x}_i\|^2 = p
\end{equation*}
\end{enumerate}

<!---------------------------------------------------------------------------->

\section*{$\blacktriangleright$~Exercise 2}

Define $\mathbf{X}$ as a $N \times p$ data matrix with $\mathbf{x}_i$ vectors on its rows. 

Define also the vector $\mathbf{y} \in \mathbb{R}^N$ containing the observations $y_i$.

Suppose that both the features and the observations have been re-centered so to have zero mean.

\begin{enumerate}
\item[(a)] Show that the intercept of a multiple linear regression using this dataset will necessarily be zero.
\item[(b)] Use the singular value decomposition (SVD) of $\mathbf{X}$ to write an expression for the parameters $\boldsymbol{\hat{\beta}}$ of the multiple linear regression model 
\begin{equation*}
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
\end{equation*}
\end{enumerate}
Consider now that we project the data matrix onto a subspace spanned by the $q$-top principal components of the data matrix $\mathbf{X}$ with $q < p$. Call this new data matrix $\mathbf{Z}$.
\begin{enumerate}
\item[(c)] Use the SVD of $\mathbf{Z}$ to write an expression for the parameters $\boldsymbol{\hat{\gamma}}$ of the multiple linear regression model 
\begin{equation*}
\mathbf{y} = \mathbf{Z}\boldsymbol{\gamma} + \boldsymbol{\varepsilon}
\end{equation*}
\item[(d)] Compare and interpret the expressions obtained in exercises (b) and (c).
\end{enumerate}

<!---------------------------------------------------------------------------->

\section*{$\blacktriangleright$~Exercise 3}

We consider the dataset \code{cars04}, which describes several properties of
different car models in the market in 2004. Each observation (i.e. car) is described by 11 features (i.e. properties) listed in Table 1.The goal of this exercise is to summarize and to interpret the data \code{cars04} using PCA. 

\begin{table}[!hbt]
\begin{tabular}{ll}
Variable & Meaning\\
\hline
\code{Retail}  & Builder recommended price(US\$)\\
\code{Dealer}  & Seller price (US\$)\\
\code{Engine}  & Motor capacity  (liters)\\
\code{Cylinders}  & Number of cylinders in the motor\\
\code{Horsepower}  &Engine power\\
\code{CityMPG}  & Consumption in city (Miles or gallon; proportional to km/liter)\\
\code{HighwayMPG}  & Consumption on roadway  (Miles or gallon)\\
\code{Weight}  & Weight (pounds)\\
\code{Wheelbase}  & Distance between front and rear wheels (inches)\\
\code{Length}  & Length (inches)\\
\code{Width}   & Width  (inches)\\
\hline
\end{tabular}
\caption{Variable list for \code{cars04}}
\label{tab:cars04:varlist}
\end{table}

Using `python` we run the following instructions:
```{python, results='hide' }
# first import the dataset
import pandas as pd
filename = './cars04.csv'
df = pd.read_csv(filename, index_col=0)
X = df.values[:, 7:]

# run scikit-learn methods
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
scl = StandardScaler()
pca = PCA()
est = make_pipeline(scl, pca)
est.fit(X)
```
\begin{enumerate}
\item[(a)] Explain what the code above does. What is the role and effect of the \code{StandardScaler}?
\end{enumerate} 

After running the following lines, we get the table below:
```{python}
for i in range(X.shape[1]):
    variance = pca.explained_variance_[i]
    variance_ratio = pca.explained_variance_ratio_[i]
    print(f'PC{i+1:02d}:', f'{variance:.3f}', f'{variance_ratio:.3f}')
```

\begin{enumerate}
\item[(b)] Are the first two principal components enough to summarize most of the information (i.e. variance) of the dataset? Justify in terms of the proportion of the total variance that they represent.
\end{enumerate} 

Principal components are linear combinations of the 11 variables from the dataset, which are printed using the lines below:
```{python}
df_pc = pd.DataFrame()
df_pc['PC1'] = pca.components_[0, :]
df_pc['PC2'] = pca.components_[1, :]
df_pc.index = df.columns[7:]
print(df_pc)
```

\begin{enumerate}
\item[(c)] How would you interpret these new variables in terms of the initial features of the dataset?
\item[(d)] The left panel of the figure below portrays the correlation plot of the PCA as described in class. Recall how it is constructed and then interpret each of the quadrants for the current dataset.
\item[(e)] Based on the projections of the data points on the first two principal components shown on the right panel of the figure below, describe which kind of car Audi RS 6, Ford Expedition 4.6 XLT and Nissan Sentra 1.8 are.
\end{enumerate} 

```{python, echo=FALSE, results='hide'}
import matplotlib.pyplot as plt
import numpy as np

X_pca = est.fit_transform(X)

fig, axs = plt.subplots(figsize=(12.0, 5.5), ncols=2)

ax = axs[0]

std_y = pca.singular_values_ / np.sqrt(len(df))
var_x = np.ones(len(std_y))  # the data is being scaled

for i in range(X.shape[1]):
    point = df_pc.iloc[i].values
    if i == 0: # Retail
        ax.text(std_y[0]/var_x[0]*point[0] - 0.32,
                std_y[1]/var_x[1]*point[1],
                df_pc.index[i], color='k', size=14)        
    elif i == 1: # Dealer
        ax.text(std_y[0]/var_x[0]*point[0] - 0.30,
                std_y[1]/var_x[1]*point[1] - 0.12,
                df_pc.index[i], color='k', size=14) 
    elif i in [2, 3, 4, 7, 10]: # Engine, Cylinders, Horsepower, Weight, Width
        ax.text(std_y[0]/var_x[0]*point[0] - 0.35,
                std_y[1]/var_x[1]*point[1],
                df_pc.index[i], color='k', size=14)                
    elif i == 8: # Wheelbase
        ax.text(std_y[0]/var_x[0]*point[0] + 0.03,
                std_y[1]/var_x[1]*point[1] + 0.08,
                df_pc.index[i], color='k', size=14) 
    elif i == 9: # Length
        ax.text(std_y[0]/var_x[0]*point[0] + 0.06,
                std_y[1]/var_x[1]*point[1],
                df_pc.index[i], color='k', size=14) 
    elif i == 5: # CityMPG
        ax.text(std_y[0]/var_x[0]*point[0] - 0.25,
                std_y[1]/var_x[1]*point[1] + 0.10,
                df_pc.index[i], color='k', size=14)         
    elif i == 6: # HighwayMPG
        ax.text(std_y[0]/var_x[0]*point[0] - 0.50,
                std_y[1]/var_x[1]*point[1] - 0.15,
                df_pc.index[i], color='k', size=14)         
    else:
        ax.text(std_y[0]/var_x[0]*point[0],
                std_y[1]/var_x[1]*point[1],
                df_pc.index[i], color='k', size=14)                
    ax.scatter(std_y[0]/var_x[0]*point[0],
            std_y[1]/var_x[1]*point[1],
            s=120, edgecolor='k', facecolor='none')        
ax.axvline(x=0, c='k', lw=0.8)
ax.axhline(y=0, c='k', lw=0.8)
ax.set_xlim(-1, +1)
ax.set_ylim(-1, +1)
circle = plt.Circle((0, 0), 1, color='k', fill=False)
ax.add_patch(circle)

ax = axs[1]
ax.axvline(x=0, c='k', lw=0.8)
ax.axhline(y=0, c='k', lw=0.8)
for i, Xi in enumerate(X_pca):
    if i in [22, 122, 279]:
        pass
    else:
        ax.text(Xi[0], Xi[1], df.index[i], size=2)
for j in [22, 112, 279]:
    ax.text(X_pca[j, 0], X_pca[j, 1], df.index[j], size=12, c='r')
ax.set_xlim(-6, 12)
ax.set_ylim(-3.5, 4)
ax.set_xlabel('PC1')
ax.set_ylabel('PC2')

fig.show()

```

\section*{$\blacktriangleright$~Exercise 4}
In this exercise, we will use the results from a survey performed in the 1950s in France. The dataset contains the average number of Francs spent on several categories of food products according to social class and the number of children per family. We display below some of the rows and columns of this dataset.

```{python}
df = pd.read_csv('foodFrance.csv', index_col=0)
print(df)
```

\begin{enumerate}
\item[(a)] Given how the dataset is defined, if we were to do a PCA, would it be preferable to scale or not the variables? Explain your reasoning.
\item[(b)] The plots below illustrate the results of the PCA carried out on the dataset. Interpret what information each principal axis convey and how it is related to the different social classes for each data point. Note that the acronyms on the right panel indicate the social class and the number of children, for instance: \code{WC4} means ``White collar with 4 children''.
\end{enumerate} 

```{python, echo=FALSE, results='hide'}
X = df.values[:, 2:].astype(np.float64)
acronyms = ['BC2', 'WC2', 'UC2', 'BC3', 'WC3', 'UC3', 'BC4', 'WC4', 'UC4', 'BC5', 'WC5', 'UC5']

pca = PCA(n_components=2)
Xpca = pca.fit_transform(X)

df_pc = pd.DataFrame()
df_pc['PC1'] = pca.components_[0, :]
df_pc['PC2'] = pca.components_[1, :]
df_pc.index = df.columns[2:]

fig, axs = plt.subplots(figsize=(12.0, 5.5), ncols=2)

ax = axs[0]
circle = plt.Circle((0, 0), 1, color='C0', lw=2.0, fill=False)
ax.add_patch(circle)
std_y = pca.singular_values_ / np.sqrt(len(df))
std_x = np.std(X, axis=0)
points = df_pc.multiply(std_y, axis=1).divide(std_x, axis=0).values
for i, pi in enumerate(points):
    # ax.scatter(pi[0], pi[1], s=120, edgecolor='k', facecolor='none')
    ax.scatter(pi[0], pi[1], s=120, color='C3')
ax.text(points[0][0] - 0.35, points[0][1] - 0.01, 'Bread', size=14)   
ax.text(points[1][0] - 0.60, points[1][1], 'Vegetables', size=14)   
ax.text(points[2][0] - 0.32, points[2][1], 'Fruits', size=14)   
ax.text(points[3][0] - 0.30, points[3][1] + 0.07, 'Meat', size=14)   
ax.text(points[4][0] - 0.35, points[4][1] - 0.10, 'Poultry', size=14)   
ax.text(points[5][0] - 0.02, points[5][1] - 0.14, 'Milk', size=14)   
ax.text(points[6][0] + 0.05, points[6][1] + 0.03, 'Wine', size=14)   
ax.axvline(x=0, c='k', lw=0.8)
ax.axhline(y=0, c='k', lw=0.8)
ax.set_xlim(-1.05, +1.05)
ax.set_ylim(-1.05, +1.05)
ax.set_title('Variable space')

ax = axs[1]
colors = {'Blue collar': 'b', 'White collar': 'g', 'Upper class': 'r'}
for socialclass in colors.keys():
    idx = (df['Class'] == socialclass)
    ax.scatter(Xpca[idx, 0], Xpca[idx, 1], s=100, c=colors[socialclass])
for i, Xi in enumerate(Xpca):
    ax.text(Xi[0] + 15, Xi[1] + 10, s=acronyms[i], size=14)
ax.axvline(x=0, c='k', lw=0.8)
ax.axhline(y=0, c='k', lw=0.8)
ax.set_xlim(-720, 1250)
ax.set_ylim(-250, 320)
ax.set_xlabel('PC1')
ax.set_ylabel('PC2')
ax.set_title('Data space')

fig.show()
```



