---
# title: "Template Work/labsheet"
# author: "Christophe Dutang, Pedro Rodrigues"
documentclass: article
papersize: a4
geometry: top=1.5cm, bottom=2cm, left=1.5cm, right=1.5cm
fontsize: 11pt
output: 
  pdf_document:
    extra_dependencies: ["enumitem"]
    number_sections: true
    toc: false
    keep_tex: false
    includes:
      in_header: "TD2-preamble.tex"
      before_body: "TD2-header.tex"
      
---

<!-- see help at https://bookdown.org/yihui/rmarkdown-cookbook/latex-output.html -->

```{r setup, include=FALSE, message=FALSE}
#see full list >knitr::opts_chunk$get()
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
library(reticulate)
use_python("/opt/homebrew/Caskroom/miniforge/base/envs/isla2025/bin/python", required = T)
```

\section*{$\blacktriangleright$~Exercise 1 (credits to EPFL CS-433)}
Assume we are doing linear regression with mean-squared loss and L2-regularization on four one-dimensional data points. Our prediction model can be written as $f(x) = a x + b$ and the optimization problem can be written as
$$
a^\star, b^\star = \underset{a, b}{\text{argmin}}~\sum_{i = 1}^4 \Big(y_i - f(x_i)\Big)^2 + \lambda a^2
$$
Assume that our data points $(x_i, y_i)$ are $\{(-2, 1), (-1, 3), (0, 2), (3, 4)\}$. What is the optimal value for the bias, $b^\star$?
\begin{itemize}
 \renewcommand{\labelitemi}{\scriptsize$\square$}
\item[(A)] Depends on the value of $\lambda$.
\item[(B)] 3
\item[\textbf{\textcolor{blue}{(C)}}] 2.5
\item[(D)] None of the above answers.
\end{itemize}

\textcolor{blue}{
The objective function can be rewritten as
$$
\mathcal{L}(a, b) = \sum_{i = 1}^4 \Big(y_i - (ax_i + b)\Big)^2 + \lambda a^2
$$
so the partial derivative with respect to $b$ is
$$
\dfrac{\partial \mathcal{L}}{\partial b} = -2\sum_i\Big(y_i - (ax_i + b)\Big) 
$$
which is zero if, and only if,
$$
\sum_i y_i = a{\sum_i x_i} + 4 b 
$$
But since $\sum_i x_i = 0$ then $b^\star = \dfrac{\sum_i y_i}{4} = 2.5$
}

\section*{$\blacktriangleright$~Exercise 2 (credits to Berkeley CS-189)}
In the following statements, the word "bias" is referring to the bias-variance decomposition. Which one of them is true?
\begin{itemize}
\item[(A)] A model trained with $N$ training points is likely to have lower variance than a model trained with $2N$ training points.
\item[\textbf{\textcolor{blue}{(B)}}] If my model is underfitting, it is more likely to have high bias than high variance.
\item[(C)] Increasing the number of parameters (weights) in a model usually improves the test set accuracy.
\item[(D)] None of the above.
\end{itemize}

\section*{$\blacktriangleright$~Exercise 3 (credits to EPFL CS-433)}
Consider a regression model where data $(x, y)$ is generated by input $x \in \mathbf{R}$ uniformly sampled between $[0, 1]$ and $y = x + \varepsilon$, where $\varepsilon$ is random noise with mean 0 and variance 1. Two models are carried out for regression: model $\mathcal{A}$ is a trained quadratic function $g_{\mathcal{A}}(x, \boldsymbol{\beta}) = \beta_0 + \beta_1 x + \beta_2 x^2$ and model $\mathcal{B}$ is a constant function $g_{\mathcal{B}}(x) = \tfrac{1}{2}$. 
Compared to model $\mathcal{B}$, model $\mathcal{A}$ has
\begin{itemize}
 \renewcommand{\labelitemi}{\scriptsize$\square$}
\item[(A)] Higher bias, higher variance.
\item[\textbf{\textcolor{blue}{(B)}}] Lower bias, higher variance.
\item[(C)] Higher bias, lower variance.
\item[(D)] Lower bias, lower variance.
\end{itemize}

\section*{$\blacktriangleright$~Exercise 4}
Consider the following `python` script:
```{python, results='hide' }
import numpy as np
import pandas as pd
import statsmodels.api as sm
np.random.seed(0)
# number of variables
pt = 201
# number of predictors
p = pt - 1
# sample size
n = 30 * p
# generate data
D = np.random.randn(n, pt)
df = pd.DataFrame(data=D)
df = df.rename(columns={0:'Y'})
# do multiple linear regression
df['intercept'] = 1
model = sm.OLS(df['Y'], df.drop(columns='Y'))
results = model.fit()
print(results.summary())
```
\begin{enumerate}
\item[(a)] What does the script do? Run it on your computer.

{\color{blue}
The script first generates a dataset with $N = 6000$ data points, where each $y_i = \varepsilon_i$ with $\varepsilon_i \sim \mathcal{N}(0, 1)$ and all predictors are independent with each other with $x_{ij} \sim \mathcal{N}(0, 1)$. The script then estimates a linear regression model relating the $y_i$ with the $x_i$, despite the fact that they are certainly not related, as we can see from the way we generated them.
}

\item[(b)] What is the true distribution of the random variable $Y$ given the first 200 columns of matrix $D$, which we shall call $X_1, \dots, X_{200}$?

{\color{blue}
$$Y \mid X_1, \dots, X_{200} \sim \mathcal{N}(0, 1)$$
}

\item[(c)] Write an equation defining the model estimated by \code{model.fit()}. What is the difference between this model and the one defined above?

{\color{blue}
The model that \code{python} estimates looks like the following: for each data point $i$,
$$
y_i = \hat{\beta}_0 + \sum_{i = 1}^{200}\hat{\beta}_i x_{ij} 
$$
}

\item[(d)] Using \code{results.pvalues} count how many estimated parameters have a $p$-value under 0.05. What is going on?
\end{enumerate}

```{python}
print(np.sum(results.pvalues < 0.05))
```

\textcolor{blue}{
This is a clear demonstration of what is commonly called the "multiple comparison problem" in the statistics literature. Since each individual statistical test has a 0.05 significance level, the fact of doing 200 of them will, in average, reject $200 \times 0.05 = 10$ times out of simple randomness. 
}

\section*{$\blacktriangleright$~Exercise 5}
In this exercise, you will perform multiple linear regression on simulated data under different conditions. To ensure reproducibility on your results, set the seed with \code{numpy.random.seed(0)} at the beginning of your script. 

\begin{enumerate}
\item[(a)] Simulate a dataset of size $N = 1000$ of the following generating model:
\begin{equation*}
\begin{array}{rcl}
X_{1,i} &=& \varepsilon_{1,i} \\[0.25em]
X_{2,i} &=& 3X_{1,i} + \varepsilon_{2,i} \\[0.25em]
Y_{i} &=& X_{2,i} + X_{1,i} + 2 + \varepsilon_{3,i}
\end{array}
\end{equation*}
where $i \in \{1,\dots, N\}$ and the $\varepsilon_{ij}$ are independent $\mathcal{N}(0,1)$ random variables. For a given $i$, what is the distribution of $(X_{1,i}, X_{2,i})$? Plot the clouds of points of the simulated values of $(X_{1,i}, X_{2,i})_{i=1, \dots, n}$. What is its shape? Can you write an analytical formula for it?
\end{enumerate}

```{python}
import numpy as np
import matplotlib.pyplot as plt
N = 1000
X = np.zeros((N, 2))
X[:, 0] = np.random.randn(N)
X[:, 1] = 3 * X[:, 0] + np.random.randn(N)
Y = X[:, 1] + X[:, 0] + 2 + np.random.randn(N)
fig, ax = plt.subplots(figsize=(3, 3))
ax.scatter(X[:, 0], X[:, 1], c='k', s=10)
ax.set_xlabel('X1')
ax.set_ylabel('X2')
fig.show()
```
\textcolor{blue}{First notice that $X_1 \sim \mathcal{N}(0, 1)$ and $X_2 = 3X_1 + \varepsilon$ with $\varepsilon \sim \mathcal{N}(0, 1)$ so writing all this in matrix notation we have:
$$
\left[\begin{array}{c}
X_1 \\ X_2
\end{array}\right] = \left[\begin{array}{cc}
1 & 0 \\
3 & 1
\end{array}\right] \left[\begin{array}{c}
Z_1 \\ Z_2
\end{array}\right] \quad \text{with} \quad \left[\begin{array}{c}
Z_1 \\ Z_2 
\end{array}\right] \sim \mathcal{N}\left(\left[\begin{array}{c}
0 \\ 0
\end{array}\right], \left[\begin{array}{cc}
1 & 0 \\ 0 & 1
\end{array}\right]\right)
$$
so that we get 
$$
\left[\begin{array}{c}
X_1 \\ X_2
\end{array}\right] \sim \mathcal{N}\left(\left[\begin{array}{c}
0 \\ 0
\end{array}\right], \left[\begin{array}{cc}
1 & 3 \\ 3 & 10
\end{array}\right]\right)
$$
}
\begin{enumerate}
\item[(b)] Let us consider the following two regression models:
\begin{equation*}
\begin{array}{rrcl}
\text{Model A:} & Y_i & = & \alpha_1 X_{1,i} + \alpha_0 + \tilde{\varepsilon}_{A,i} \\[0.5em]
\text{Model B:} & Y_i & = & \beta_2 X_{2,i} + \beta_0 + \tilde{\varepsilon}_{B,i}
\end{array}
\end{equation*}
where $\tilde{\varepsilon}_{A,i} \sim \mathcal{N}(0, \sigma_A^2)$ and $\tilde{\varepsilon}_{B,i} \sim \mathcal{N}(0, \sigma_B^2)$. What should be the values of $\hat{\alpha}_0, \hat{\alpha}_1, \hat{\sigma}_A^2, \hat{\beta}_0, \hat{\beta}_2, \hat{\sigma}_B^2$ when $N \to \infty$? Consider $N = 1000$ and check whether the estimates of the parameters are close to the true values that you've calculated. Now do \code{np.random.seed(3)} and simulate again a dataset $X_{1,i}, X_{2,i}, Y_i$ for $n = 10$. Estimate the parameters. What happens?
\end{enumerate}

\textcolor{blue}{
Recall that the true model of the observations is written as:
$$
Y = 2 + X_1 + X_2 + \varepsilon
$$
Exploring the way that the predictors are generated, we can rewrite the true model so to appear only the variable $X_1$:
$$
Y = 2 + X_1 + X_2 + \varepsilon \iff Y = 2 + X_1 + (3X_1 + \varepsilon_1) + \varepsilon 
$$
which indicates that when $N \to \infty$ the model $\mathcal{A}$ will converge in such a way that:
$$
\alpha_1 = \lim_{N \to \infty}\hat{\alpha}_1 = 4 \quad \text{and} \quad \alpha_0 = \lim_{N \to \infty}\hat{\alpha}_0 = 2 \quad \text{and} \quad \sigma^2_A = \lim_{N \to \infty}\hat{\sigma}^2_A = 2
$$
Similarly, we can rewrite things so to get a model depending only of $X_2$ as per
$$
Y = 2 + \Big(\dfrac{X_2}{3} - \dfrac{\varepsilon_2}{3}\Big) + \varepsilon = 2 + \dfrac{4}{3}X_2 + \Big(\varepsilon - \dfrac{\varepsilon_2}{3}\Big)
$$
which indicates that when $N \to \infty$ the model $\mathcal{A}$ will converge in such a way that:
$$
\beta_2 = \lim_{N \to \infty}\hat{\beta}_2 = \dfrac{4}{3} \quad \text{and} \quad \beta_0 = \lim_{N \to \infty}\hat{\beta}_0 = 2 \quad \text{and} \quad \sigma^2_B = \lim_{N \to \infty}\hat{\sigma}^2_B = \dfrac{10}{9}
$$
Running the following script we get the estimates for $N = 10^4$
}

```{python}
N = 10_000
X = np.zeros((N, 2))
X[:, 0] = np.random.randn(N)
X[:, 1] = 3 * X[:, 0] + np.random.randn(N)
Y = X[:, 0] + X[:, 1] + 2 + np.random.randn(N)

df = pd.DataFrame()
df['Y'] = Y
df['intercept'] = np.ones(N)
df['X1'] = X[:, 0]
model_A = sm.OLS(df['Y'], df[['intercept', 'X1']])
results = model_A.fit()
print('model A')
print(results.params)
print('sigma2_A = ', results.scale)
print('')
df = pd.DataFrame()
df['Y'] = Y
df['intercept'] = np.ones(N)
df['X2'] = X[:, 1]
model_B = sm.OLS(df['Y'], df[['intercept', 'X2']])
results = model_B.fit()
print('model B')
print(results.params)
print('sigma2_B = ', results.scale)
```

\begin{enumerate}
\item[(c)] Let us now consider the full model
$$
Y_i = \gamma_2 X_{2, i} + \gamma_1 X_{1, i} + \gamma_0 + \varepsilon_i
$$
where $i \in \{1,\dots, n\}$ and the $\varepsilon_i$ are independent $\mathcal{N}(0, \sigma^2)$ random variables. For the previously simulated data with $n = 10$, estimate $\hat{\gamma}_0, \hat{\gamma}_1, \hat{\gamma}_2, \hat{\sigma}^2$ and compare them with the parameters obtained in item (b). What can you say about the effects of $X_1$ and $X_2$ on $Y$? And about their correlation?
\end{enumerate}

\textcolor{blue}{
Running the estimation with the full model but only $N = 10$:
}
```{python}
N = 10
X = np.zeros((N, 2))
X[:, 0] = np.random.randn(N)
X[:, 1] = 3 * X[:, 0] + np.random.randn(N)
Y = X[:, 0] + X[:, 1] + 2 + np.random.randn(N)

df = pd.DataFrame()
df['Y'] = Y
df['intercept'] = np.ones(N)
df['X1'] = X[:, 0]
df['X2'] = X[:, 1]
model = sm.OLS(df['Y'], df[['intercept', 'X1', 'X2']])
results = model.fit()
print('full model')
print(results.params)
print('sigma2 = ', results.scale)
```

\textcolor{blue}{We see that the estimates are quite far from the true values. We can also inspect the summary to see the standard errors of the estimates:}

```{python}
print(results.summary())
```