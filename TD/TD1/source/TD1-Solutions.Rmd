---
# title: "Template Work/labsheet"
# author: "Christophe Dutang, Pedro Rodrigues"
documentclass: article
papersize: a4
geometry: top=1.5cm, bottom=2cm, left=1.5cm, right=1.5cm
fontsize: 11pt
output: 
  pdf_document:
    extra_dependencies: ["enumitem"]
    number_sections: true
    toc: false
    keep_tex: false
    includes:
      in_header: "TD1-preamble.tex"
      before_body: "TD1-header.tex"
      
---

<!-- see help at https://bookdown.org/yihui/rmarkdown-cookbook/latex-output.html -->

```{r setup, include=FALSE, message=FALSE}
#see full list >knitr::opts_chunk$get()
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
```

\section*{$\blacktriangleright$~Exercise 1}

Let $U$ and $V$ be two independent random variables with uniform distribution
over $[0,1]$. 

Let $X = U+V$ and $Y=U-V$. 

\begin{enumerate}
\item[(a)] Compute the expectation and covariance matrix of $Z = \left( 
\begin{array}{cc} X & Y\end{array} \right)^T$. %$Z = (X, Y)^T$.

{\color{blue}
$Z = \left[\begin{array}{c}X \\ Y\end{array}\right] \Rightarrow \mathbb{E}[Z] = \left[\begin{array}{c}\mathbb{E}[X] \\ \mathbb{E}[Y]\end{array}\right] = \left[\begin{array}{c}\mathbb{E}[U + V] \\ \mathbb{E}[U - V]\end{array}\right] = \left[\begin{array}{c}\mathbb{E}[U] + \mathbb{E}[V]  \\ \mathbb{E}[U] - \mathbb{E}[V]\end{array}\right]$

Remember that
$$
\mathbb{E}[U] = \int_{\mathbb{R}}u~p_U(u)\mathrm{d}u = \int_{0}^1u~\mathrm{d}u = \dfrac{1}{2} = \mathbb{E}[V] \quad \text{therefore} \quad \mathbb{E}[Z] = \left[\begin{array}{c}1 \\ 0\end{array}\right]
$$
We also have that
$$
\text{cov}(Z) = \mathbf{\Sigma}_Z = \mathbb{E}\Big[\big(Z-\mathbb{E}[Z]\big)\big(Z-\mathbb{E}[Z]\big)^\top\Big] = \left[\begin{array}{cc}\text{Var}(X) & \text{Cov}(X, Y) \\ \text{Cov}(X, Y) & \text{Var}(Y)\end{array}\right]
$$
Since $U$ and $V$ are independent, we can write
$$
\begin{array}{rcl}
\text{Var}(X) &=& \text{Var}(U + V) = \text{Var}(U) + \text{Var}(V) \\[0.5em]
\text{Var}(Y) &=& \text{Var}(U - V) = \text{Var}(U) + \text{Var}(V) \\[1em]
\end{array}
$$
and remember the definition of variance
$$
\text{Var}(U) = \int_{\mathbb{R}}(u - \mathbb{E}[u])^2~p_U(u)\mathrm{d}u = \dfrac{1}{12} = \text{Var}(V)
$$
Now we have to calculate the cross-covariance,
$$
\text{Cov}(X, Y) = \mathbb{E}_{XY}\Big[\big(X - \mathbb{E}[X]\big)\big(Y - \mathbb{E}[Y]\big)\Big] = \mathbb{E}\Big[\big(X - 1\big)Y\Big] = \mathbb{E}\Big[XY\Big] - \mathbb{E}[Y] = \mathbb{E}[U^2 - V^2]
$$
and finally
$$
\text{Cov}(X, Y) = \mathbb{E}[U^2] - \mathbb{E}[V^2] = 0 \quad \text{therefore} \quad \mathbf{\Sigma}_Z =\left[\begin{array}{cc}\frac{1}{6} & 0 \\ 0 & \frac{1}{6}\end{array}\right]
$$}

\item[(b)] Prove that $X$ and $Y$ are uncorrelated but not independent.

{\color{blue}
From the result in (a) we see that $\text{Cov}(X, Y) = 0$ so they are indeed uncorrelated.

To check whether two random variables are independent we have to first calculate their joint pdf $p_{XY}(x, y)$ and compare it to the marginal pdfs $p_X(x)$ and $p_Y(y)$. The random variables will be independent **if, and only if,** we can write $p_{XY}(x, y) = p_X(x) p_Y(y)$

First, recall that
$$
\begin{array}{rcl}
X &=& U + V \\
Y &=& U - V
\end{array} \quad \iff \quad \begin{array}{rcl}
U &=& \tfrac{1}{2}(X + Y) \\
Y &=& \tfrac{1}{2}(X - Y)
\end{array}
$$
So let's first check what is the joint pdf. We can simply use the transformation method, which writes
$$
p_{XY}(x, y) = p_{UV}(u, v) \times |\det(J)|^{-1}
$$
where $J$ is the jacobian of the transformation from $(U, V)$ to $(X, Y)$:
$$
J = \left[\begin{array}{cc}
\dfrac{\partial X}{\partial U} & \dfrac{\partial X}{\partial V} \\[0.5em]
\dfrac{\partial Y}{\partial U} & \dfrac{\partial Y}{\partial V}
\end{array}\right] = \left[\begin{array}{cc}
1 & 1 \\[0.5em]
1 & -1
\end{array}\right] \Rightarrow |\text{det}(J)| = 2
$$
Then
$$
p_{XY}(x, y) = p_U\Big(\tfrac{1}{2}(x+y)\Big)p_V\Big(\tfrac{1}{2}(x-y)\Big)\times \dfrac{1}{2}
$$
We thus identify a pdf which is constant on a region $\mathcal{S}$ defined by the two uniform marginals. Let's see what $\mathcal{S}$ looks like:

Since $0 \leq U \leq 1$ and $0 \leq V \leq 1$, then we have $0 \leq X + Y \leq 2$ and $0 \leq X - Y \leq 2$, therefore $0 \leq X \leq 2$ and $-X \leq Y \leq X$

So we can write the joint pdf as per
$$
p_{XY}(x, y) = \left\{
\begin{array}{ccl}
\dfrac{1}{2}, & 0 \leq x \leq 2 & -x \leq y \leq x \\[1em]
0, & \text{otherwise}
\end{array}
\right.
$$
OK, we have the joint pdf. Let's see now what the marginal for $X$ looks like.

Remember that $X = U + V$ with $U$ and $V$ independent. One way of calculating the pdf for the sum of two general RVs is to begin with the CDF and then taking its derivative. Let's see where this gets us:
$$
F_X(x) = \mathbb{P}(X \leq x) = \mathbb{P}(U + V \leq x) = \underset{u + v \leq x}{\iint}p_{UV}(u, v)\mathrm{d}u\mathrm{d}v = \int_{-\infty}^{+\infty}\int_{-\infty}^{x - u}p_U(u)p_V(v)\mathrm{d}u\mathrm{d}v
$$
Rearranging things, we get
$$
F_X(x) = \int_{-\infty}^{+\infty}p_U(u)\Bigg(\int_{-\infty}^{x - u}p_V(v)\mathrm{d}v\Bigg)\mathrm{d}u = \int_{-\infty}^{+\infty}p_U(u)F_V(x -u)\mathrm{d}u
$$
So the pdf of $X$ can be calculated as
$$
p_X(x) = \dfrac{d}{dx}F_X(x) = \int_{-\infty}^{+\infty}p_U(x) \Bigg(\dfrac{d}{dx}F_V(x - u)\Bigg)\mathrm{d}u = \int_{-\infty}^{+\infty}p_U(x) p_V(x - u)\mathrm{d}u
$$
For our specific case with two uniform distributions, this gives
$$
p_X(x) = \int_{0}^{1}p_V(x - u)\mathrm{d}u = \int_{0}^{1}\mathbf{1}_{[0, 1]}(x - u)\mathrm{d}u = \left\{\begin{array}{rl}
0 & x \leq 0 \quad \text{or} \quad x \geq 2 \\
x & 0 \leq x \leq 1\\
2 - x & 1 \leq x \leq 2
\end{array}\right.
$$
We see right away that $p_X(x)$ is not a constant, so we don't even need to calculate the marginal for $Y$ to conclude that the we can not factorize the joint pdf into the marginals. Conclusion, the two RVs are not independent.
}

\end{enumerate}

<!---------------------------------------------------------------------------->

\section*{$\blacktriangleright$~Exercise 2}

Let $Z = \left( \begin{array}{cc} X & Y\end{array} \right)^T$ be a Gaussian
vector with mean $\mu = \left( \begin{array}{cc} 1 & 2\end{array} \right)^T$ 
and covariance $\Sigma = \left( \begin{array}{cc} 1 &-1\\ -1 &2\end{array} 
\right)$.

\begin{enumerate}
\item[(a)] Compute the probability density function of $Z$.

{\color{blue}
The pdf of a bivariate normal distribution is
$$
p_Z(\mathbf{z}) = \dfrac{1}{2\pi \det({\Sigma})}\exp\left(-\dfrac{1}{2}(\mathbf{z} - \mu)^\top \Sigma^{-1}(\mathbf{z} - \mu)\right)
$$
With $\det(\Sigma) = 1$ and $\Sigma^{-1} = \left[\begin{array}{cc}2 & 1 \\ 1 & 1\end{array}\right]$

We end up with
$$
p_Z(x, y) = \dfrac{1}{2\pi}\exp\left(-\dfrac{1}{2}(2x^2 + y^2 + 2xy - 8x -6y + 10)\right)
$$
}
\item[(b)] Using $$f_{Y|X=x}(y) = {\frac{f_{(X,Y)}(x,y)}{f_{X}(x)}}$$ compute
the distribution of $Y$ given $X=x$.

{\color{blue}
The conditional expectation can be written as
$$
p_{Y \mid X = x}(y) = \dfrac{p_{XY}(x, y)}{p_X(x)} \quad \text{with} \quad p_X(x) = \dfrac{1}{\sqrt{2 \pi}}\exp\left(-\dfrac{1}{2}(x-1)^2\right)
$$
Which after lots of simplifications gives us:
$$
p_{Y \mid X = x}(y) = \dfrac{1}{\sqrt{2 \pi}}\exp\left(-\dfrac{1}{2}\Big(y-(3 -x)\Big)^2\right) = \mathcal{N}(y~|~3-x, 1)
$$
}
\item[(c)] What is the best prediction of $Y$ given $X=x$?

{\color{blue}
Remember from the CM1 that the best prediction of $Y$ given $X = x$ is the conditional expectation as per
$$
\hat{Y} = \mathbb{E}\Big[Y~|~X = x\Big] = 3 -x
$$
}

\end{enumerate}

\section*{$\blacktriangleright$~Exercise 3}

Consider the regression problem discussed in class: we want to determine a 
function $\mu$ that takes a predictor $X$ as input and gives the best estimate
in terms of mean squared error for the observed variable $Y$. 

In mathematical terms, we have an optimization problem defined as
$$
\mu = \underset{f \in \mathcal{F}}{\text{argmin}}~\mathbb{E}_{(X, Y)}\Big[\Big(Y - f(X)\Big)^2\Big]
$$
where $\mathcal{F}$ is a space of functions with finite squared norm.

Show that the solution is $\mu(x) = \mathbb{E}_{Y \mid X}\Big[Y \mid X = x\Big]$

\textcolor{blue}{We can rewrite the loss function to minimize as per}
$$
{\color{blue}
\begin{array}{rcl}
\mathbb{E}_{(X, Y)}\Big[(Y - f(X))^2\Big] &=& \mathbb{E}_X\Big[\mathbb{E}_{Y|X}\big[(Y - f(X))^2~|~X\big]\Big] \\[0.5em] 
&=& \mathbb{E}_X\Big[\text{Var}(Y - f(X)~|~X) +  \Big(\mathbb{E}_{Y|X}\big[(Y - f(X)~|~X]\Big)^2\big]\\[0.5em]
&=& \mathbb{E}_X\Big[\text{Var}(Y~|~X) +  \Big(\mathbb{E}_{Y|X}\big[(Y - f(X)~|~X]\Big)^2\Big]\\[0.5em]
&\geq& \mathbb{E}_X\Big[\text{Var}(Y~|~X)\Big]
\end{array}}
$$
\textcolor{blue}{so we see that to minimize the loss we should choose $f(x) = \mathbb{E}_{Y|X}[Y~|~X=x]$}

\section*{$\blacktriangleright$~Exercise 4}
Consider the Gaussian simple linear regression model presented in class
$$
Y = \beta_0 + \beta_1 X_1 + \varepsilon \quad \text{with} \quad \varepsilon \sim \mathcal{N}(0, \sigma^2)
$$
The estimates for the parameters of the model, $\hat{\beta}_0$ and $\hat{\beta}_1$, are obtained $N$ paired samples $(x_i, y_i)$.
\begin{enumerate}
\item[(a)] Show that the estimated parameters are unbiased.

{\color{blue}
From the CM we have the expressions

$\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X}$ and $\hat{\beta}_1 = \dfrac{c_{XY}}{s_X^2}$ with $c_{XY} = \dfrac{1}{N}\sum_{i = 1}^N (x_i - \bar{x})(y_i - \bar{y})$ and $s_X^2 = \dfrac{1}{N}\sum_{i = 1}^N (x_i - \bar{x})^2$

We first check the unbiasedness of $\hat{\beta}_1$ 

$\mathbb{E}\big[\hat{\beta}_1\big] = \mathbb{E}_{X_1, \dots, X_N}\Big[\mathbb{E}\big[\hat{\beta}_1 \mid X_1 = x_1, \dots, X_N = x_N\big]\Big]$ (explain why we first consider the conditional expectation)

$\mathbb{E}\big[\hat{\beta}_1 \mid X_1 = x_1, \dots, X_N = x_N\big] = \dfrac{1}{s_X^2}\dfrac{1}{N}\displaystyle\sum_{i = 1}^N \Big(x_i - \bar{x}\Big)\Big(\mathbb{E}[Y_i~|~X_i = x_i] - \mathbb{E}[\bar{y}~|~X_1=x_1, \dots, X_N = x_N]\Big)$ (the $X$ are fixed but the $Y$ are random variables)

Note that,
$$
\mathbb{E}\Big[Y_i~|~X_i = x_i\Big] = \beta_0 + \beta_1 x_i + \mathbb{E}[\varepsilon_i] = \beta_0 + \beta_1 x_i
$$
and that
$$
\mathbb{E}\Big[\bar{y}~|~X_1 = x_1, \dots, X_N = x_N\Big] = \mathbb{E}\Bigg[\dfrac{1}{N}\sum_{i = 1}^N (\beta_0 + \beta_1 x_i + \varepsilon_i)\Bigg] = \beta_0 + \beta_1 \bar{x}
$$
so we get
$$
\mathbb{E}\big[\hat{\beta}_1 \mid X_1 = x_1, \dots, X_N = x_N\big] = \dfrac{1}{s_X^2}\dfrac{1}{N}\displaystyle\sum_{i = 1}^N \Big(x_i - \bar{x}\Big)\Big(\beta_0 + \beta_1 x_i - \beta_0 - \beta_1 \bar{x}\Big) = \beta_1
$$
Then taking the expectacion along all possible datasets, we get
$$
\mathbb{E}[\hat{\beta}_1] = \mathbb{E}_{X_1, \dots, X_N}\Big[\beta_1\Big] = \beta_1
$$
The bias for $\hat{\beta}_0$ is checked similarly.
$$
\mathbb{E}[\hat{\beta}_0~|~X_1=x_1, \dots, X_N = x_N] = \mathbb{E}[\bar{Y} - \hat{\beta}_1 \bar{X}~|~X_1=x_1, \dots, X_N = x_N]
$$
Which then gives us
$$
\mathbb{E}[\bar{Y}~|~X_1=x_1, \dots, X_N=x_N] = \beta_0 + \beta_1\bar{x}
$$
and
$$
\mathbb{E}[\hat{\beta}_1\bar{X}~|~{X}_1 = x_1, \dots, X_N = x_N] = \bar{x}~\mathbb{E}[\hat{\beta}_1~|~{X}_1 = x_1, \dots, X_N = x_N] = \beta_1\bar{x}
$$
so in the end we get
$$
\mathbb{E}[\hat{\beta}_0~|~X_1=x_1, \dots, X_N = x_N] = \beta_0 + \beta_1\bar{x} - \beta_1\bar{x} = \beta_0
$$
}

\item[(b)] Show that $$\text{Var}(\hat{\beta}_1) = \dfrac{\sigma^2}{N}~\dfrac{1}{s_X^2} \quad \text{and} \quad \text{Var}(\hat{\beta_0}) = \dfrac{\sigma^2}{N}\left(1+\dfrac{\bar{X}^2}{s_X^2}\right)$$
where $\bar{X} = \dfrac{1}{N}\displaystyle\sum_{i = 1}^N x_i$ and $s_X^2 = \dfrac{1}{N}\displaystyle\sum_{i = 1}^N (x_i - \bar{X})^2$.

{\color{blue}

OK, now let's get the variances.

Remember that $\hat{\beta}_1 = \dfrac{c_{XY}}{s^2_X} = \left(\dfrac{1}{N}\displaystyle\sum_i x_i y_i - \bar{x}\bar{y}\right)\dfrac{1}{s_X^2}$ so we will first rewrite it so that things get easier later

Note that
$$
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i \quad \text{and} \quad \bar{y} = \beta_0 + \beta_1 \bar{x} + \bar{\varepsilon} \quad \text{where} \quad \bar{\varepsilon} = \dfrac{1}{N}\sum_i \varepsilon_i
$$

and that
$$
\dfrac{1}{N}\sum_i x_i y_i = \dfrac{1}{N}\sum_i x_i(\beta_0 + \beta_1 x_i + \varepsilon_i) = \beta_0 \bar{x} + \beta_1 \dfrac{1}{N}\sum_i x_i^2 + \dfrac{1}{N}\sum_i x_i \varepsilon_i
$$

and
$$
\bar{x}\bar{y} = \beta_0 \bar{x} + \beta_1(\bar{x})^2 + \bar{x}\bar{\varepsilon}
$$
Then
$$
\hat{\beta}_1 = \Big(\beta_1 s_X^2 + \dfrac{1}{N}\sum_i x_i \varepsilon_i - \bar{x}\bar{\varepsilon}\Big)\dfrac{1}{s_X^2} = \beta_1 + \dfrac{1}{s_X^2}\dfrac{1}{N}\sum_i (x_i - \bar{x})\varepsilon_i
$$
This way of rewriting the estimator $\hat{\beta}_1$ is very insightful, since now we can easily write that
$$
\text{Var}(\hat{\beta}_1) = \mathbb{E}_{X_1, \dots, X_N}\Big[\text{Var}_X\big(\hat{\beta}_1~|~X_1 = x_1, \dots, X_N = x_N\big)\Big] = \mathbb{E}_{X_1, \dots, X_N}\left[\dfrac{1}{s_X^4}\dfrac{1}{N^2}\sum_i (x_i - \bar{x})^2 \sigma^2\right]
$$
so we get
$$
\text{Var}(\hat{\beta}_1) = \dfrac{1}{N^2}\dfrac{1}{s_X^4}s_X^2Ns_X^2\sigma^2 = \dfrac{1}{N}\dfrac{\sigma^2}{s_X^2}
$$
What about $\hat{\beta}_0$ ?
$$
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x} = \Big(\beta_0 + \beta_1 \bar{x} + \bar{\varepsilon}\Big) - \Big(\beta_1 + \dfrac{1}{s_X^2}\dfrac{1}{N}\sum_i (x_i - \bar{x})\varepsilon_i\Big)\bar{x}
$$
which gives us
$$
\hat{\beta}_0 = \beta_0 + \bar{\varepsilon} - \frac{1}{s_X^2}\dfrac{1}{N}\sum_i (x_i - \bar{x})\bar{x}\varepsilon_i
$$
Theferore,
$$
\text{Var}\big(\hat{\beta}_0\big) = \dfrac{\sigma^2}{N} + \dfrac{1}{N^2 s_X^4}\sum_i(x_i - \bar{x})^2 \bar{x}^2\sigma^2 = \dfrac{1}{N}\sigma^2\Big(1 + \dfrac{\bar{x}^2}{s_X^2}\Big)
$$


}

\end{enumerate}

Using the estimated parameters, we can predict that for a given arbitrary value of $X$, say $x$ (sometimes called the operation point), we have that on average $Y$ will be $$\hat{m}(x) = \hat{\beta}_0 + \hat{\beta}_1 x$$ 
\begin{enumerate}
\item[(c)] Show that 
$$\mathbb{E}\Big[\hat{m}(x)\Big] = \beta_0 + \beta_1 x$$

{\color{blue}
$$
\hat{m}(x) = \hat{\beta}_0 + \hat{\beta}_1x = \big(\bar{y} - \hat{\beta}_1\bar{x}\big) + \hat{\beta}_1x = \bar{y} + \hat{\beta}_1(x - \bar{x})
$$
Taking back the expression from item (b)
$$
\begin{array}{rcl}
\hat{m}(x) &=& \bar{y} + \Bigg(\beta_1 + \dfrac{1}{s_X^2}\dfrac{1}{N}\displaystyle\sum_i (x_i - \bar{x})\varepsilon_i\Bigg)(x - \bar{x}) \\[1em]
&=& \dfrac{1}{N}\displaystyle\sum_i \Big(\beta_0 + \beta_1 x_i +\varepsilon_i \Big) + \Big(\beta_1 + \dfrac{1}{s_X^2}\dfrac{1}{N}\sum_i (x_i - \bar{x})\varepsilon_i\Big)(x - \bar{x}) \\[1em]
&=& \beta_0 + \beta_1 \bar{x} + \bar{\varepsilon} + \beta_1 x - \beta_1 \bar{x} + \dfrac{(x- \bar{x})}{Ns_X^2}\displaystyle\sum_i(x_i - \bar{x})\varepsilon_i \\[1em]
&=& \beta_0 + \beta_1 x + \bar{\varepsilon} + \dfrac{(x- \bar{x})}{Ns_X^2}\displaystyle\sum_i(x_i - \bar{x})\varepsilon_i \\[1em]
\mathbb{E}[\hat{m}(x)] &=& \beta_0 + \beta_1 x 
\end{array}
$$
}

\item[(d)] Show that the variance of $\hat{m}(x)$ conditioned on a given choice of datapoints $x_1, \dots, x_N$ can be written as per$$\text{Var}_X\Big(\hat{m}(x)\Big) = \dfrac{\sigma^2}{N}\left(1 + \dfrac{(x - \bar{X})^2}{s_X^2}\right)$$Describe how the variance changes for different choices of the operation point.

{\color{blue}
Finally, the conditional variance on a given choice of dataset $x_1, \dots, x_N$ is
$$
\text{Var}_X\Big(\hat{m}(x)\Big) = \text{Var}_X\left(\beta_0 + \beta_1 x + \bar{\varepsilon} + \dfrac{(x- \bar{x})}{Ns_X^2}\displaystyle\sum_i(x_i - \bar{x})\varepsilon_i\right)
$$
which can be simplified as per
$$
\begin{array}{rcl}
\text{Var}_X\Big(\hat{m}(x)\Big) &=& \text{Var}\left(\dfrac{1}{N}\displaystyle\sum_i \varepsilon_i + \tfrac{(x - \bar{x})}{N s_X^2}\sum_i (x_i - \bar{x})\varepsilon_i\right) \\[1em]
&=& \text{Var}\Bigg(\dfrac{1}{N}\displaystyle\sum_i  \Big[1 + \tfrac{(x - \bar{x})}{s_X^2}(x_i - \bar{x})\Big]\varepsilon_i\Bigg) \\[1em]
&=& \text{Var}\Bigg(\dfrac{1}{N}\displaystyle\sum_i  \Big[1 + \tfrac{(x - \bar{x})}{s_X^2}(x_i - \bar{x})\Big]\varepsilon_i\Bigg) \\[1em]
&=& \dfrac{1}{N^2}\displaystyle\sum_i\Big[1 + \tfrac{(x - \bar{x})}{s_X^2}(x_i - \bar{x})\Big]^2\text{Var}(\varepsilon_i) \\[1em]
&=& \dfrac{\sigma^2}{N^2}\displaystyle\sum_i\Bigg(1 + \dfrac{2(x-\bar{x})(x_i - \bar{x})} {s_X^2} + \dfrac{(x-\bar{x})^2(x_i - \bar{x})^2} {s_X^4}\Bigg) \\[1em]
&=& \dfrac{\sigma^2}{N}\displaystyle\sum_i\Bigg(1 + \dfrac{(x-\bar{x})^2} {s_X^2}\Bigg) \\[1em]
\end{array}
$$
Notice that to get the unconditional variance we would have to take the expectation with respect to the $x_i$, which can be quite cumbersome and not that much insightful. We won't do that.
}

\end{enumerate}